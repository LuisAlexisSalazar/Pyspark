{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff1a1cfe-155f-4920-bcfa-113274442ad6",
     "showTitle": false,
     "title": ""
    },
    "id": "nB2R34EJXDYu"
   },
   "source": [
    "# Funciones de Usuario\n",
    "1. Definiendo una funcion\n",
    "1. Crear y aplicar UDF\n",
    "1. Registrar UDF para usar en SQL\n",
    "1. Usar la sintaxis Decorator (solo Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\LibreriasPython\\\\spark-3.1.2-bin-hadoop2.7\\\\python\\\\pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DXsAdVd9XGic"
   },
   "outputs": [],
   "source": [
    "#Google Colab\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# inicializamos datos\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-U9Rodi4XUtw",
    "outputId": "8b2d37a3-01b7-44e1-a920-97600dffba05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '01 Archivos Json.ipynb',\n",
       " '02 Conceptos de Spark Streaming.ipynb',\n",
       " '03_UDFs.ipynb',\n",
       " 'Laboratorio Spark Streaming.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "890bd56d-0c55-4d1f-9152-c31263d72820",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxfL5HluXDZa",
    "outputId": "8050bb11-20c8-442e-e84a-de94889b0f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------\n",
      " order_id                | 257437                                                            \n",
      " email                   | kmunoz@powell-duran.com                                           \n",
      " transaction_timestamp   | 1592194221828900                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 1995.0                                                            \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{null, M_PREM_K, Premium King Mattress, 1995.0, 1995.0, 1}]      \n",
      "-RECORD 1------------------------------------------------------------------------------------\n",
      " order_id                | 282611                                                            \n",
      " email                   | bmurillo@hotmail.com                                              \n",
      " transaction_timestamp   | 1592504237604072                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 940.5                                                             \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{NEWBED10, M_STAN_Q, Standard Queen Mattress, 940.5, 1045.0, 1}] \n",
      "-RECORD 2------------------------------------------------------------------------------------\n",
      " order_id                | 257448                                                            \n",
      " email                   | bradley74@gmail.com                                               \n",
      " transaction_timestamp   | 1592200438030141                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 945.0                                                             \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{null, M_STAN_F, Standard Full Mattress, 945.0, 945.0, 1}]       \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesPath = '../data/sales.parquet'\n",
    "\n",
    "salesDF = spark.read.parquet(salesPath)\n",
    "\n",
    "salesDF.show(3, vertical = True,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b88ff625-ffb6-4341-88dd-504db85e561e",
     "showTitle": false,
     "title": ""
    },
    "id": "1e9ZOaLEXDZi"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Con Spark puro\n",
    "\n",
    "\n",
    "Con Spark puro para obtener la primera letra de una cadena del campo `email`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MV9l9LSYYMSy",
    "outputId": "641ab39c-2f2e-43db-c569-94035cd8754a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------\n",
      " order_id                | 257437                                                            \n",
      " email                   | kmunoz@powell-duran.com                                           \n",
      " transaction_timestamp   | 1592194221828900                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 1995.0                                                            \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{null, M_PREM_K, Premium King Mattress, 1995.0, 1995.0, 1}]      \n",
      " FirstLetter             | k                                                                 \n",
      "-RECORD 1------------------------------------------------------------------------------------\n",
      " order_id                | 282611                                                            \n",
      " email                   | bmurillo@hotmail.com                                              \n",
      " transaction_timestamp   | 1592504237604072                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 940.5                                                             \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{NEWBED10, M_STAN_Q, Standard Queen Mattress, 940.5, 1045.0, 1}] \n",
      " FirstLetter             | b                                                                 \n",
      "-RECORD 2------------------------------------------------------------------------------------\n",
      " order_id                | 257448                                                            \n",
      " email                   | bradley74@gmail.com                                               \n",
      " transaction_timestamp   | 1592200438030141                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 945.0                                                             \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{null, M_STAN_F, Standard Full Mattress, 945.0, 945.0, 1}]       \n",
      " FirstLetter             | b                                                                 \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# En spark Obteniendo primera letra\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "salesDF.withColumn('FirstLetter', F.substring(F.col('email'),1, 1)).show(3,vertical = True,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b88ff625-ffb6-4341-88dd-504db85e561e",
     "showTitle": false,
     "title": ""
    },
    "id": "1e9ZOaLEXDZi"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Definiendo Funcion\n",
    "\n",
    "\n",
    "Defina una función en Python/Scala local para obtener la primera letra de una cadena del campo `email`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9ccc8220-3dd5-4fc0-acc2-07a86d11d93b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ceNnK-WmXDZl",
    "outputId": "bf49a22c-419a-4f1f-dad4-d6d42d398766"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def firstLetterFunction(email):\n",
    "  return email[0]\n",
    "\n",
    "firstLetterFunction(\"annagray@kaufman.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71b74a4c-1a97-4d9f-b555-d9efac2ad829",
     "showTitle": false,
     "title": ""
    },
    "id": "HoUMGSUIXDZ8"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Crear y aplicar UDF\n",
    "Defina una UDF que envuelva la función. Esto serializa la función y la envía a los ejecutores para poder usarla en nuestro DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "51ef0b40-4802-4ca3-8b38-83e98c4fc6b5",
     "showTitle": false,
     "title": ""
    },
    "id": "7lFTowdVXDZ-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "# Registrar la función que hemos creado de python para usarla en pyspark\n",
    "firstLetterUDF = udf(firstLetterFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b3555b1-69e8-40e2-97ea-4f07bded4b36",
     "showTitle": false,
     "title": ""
    },
    "id": "NkpJZ1-EXDZ-"
   },
   "source": [
    "Apply UDF on the `email` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca3468c0-9b5c-4006-8a79-40260a756444",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruFBJvqJXDaA",
    "outputId": "d9b46a49-04a4-4c7e-f24e-5788adcb134a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|firstLetterFunction(email)|\n",
      "+--------------------------+\n",
      "|                         k|\n",
      "|                         b|\n",
      "|                         b|\n",
      "|                         j|\n",
      "|                         w|\n",
      "+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "salesDF.select( firstLetterUDF(col(\"email\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8aca131b-0cfc-48f4-8f54-328bd909298e",
     "showTitle": false,
     "title": ""
    },
    "id": "1DYm0KzdXDaD"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png)Registrar UDF para usar en SQL\n",
    "Registre UDF usando spark.udf.register para crear UDF en el espacio de nombres SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a559e695-0da4-4553-b726-05300edf488c",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRgc24iXXDaJ",
    "outputId": "5fd0f9d5-bd7f-4151-d81f-7ff83c3e741e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.firstLetterFunction(email)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un vista temporal llamada sales\n",
    "salesDF.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# Registrar una función en sql para su uso en las queries con el nombre \"sql_udf\"\n",
    "spark.udf.register(\"sql_udf\", firstLetterFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "257c9603-fd46-4205-8356-5bc10ab457bc",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTKbjkOwXDaK",
    "outputId": "b34b3bf6-95c6-469d-8ea1-385574d30629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|firstLetter|\n",
      "+-----------+\n",
      "|          k|\n",
      "|          b|\n",
      "|          b|\n",
      "|          j|\n",
      "|          w|\n",
      "|          e|\n",
      "|          c|\n",
      "+-----------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT sql_udf(email) AS firstLetter FROM sales'\n",
    "\n",
    "\n",
    "df = spark.sql(query)\n",
    "\n",
    "\n",
    "df.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ec90fa3-7799-472e-a458-c5c3733608d3",
     "showTitle": false,
     "title": ""
    },
    "id": "HkJaJFcQXDaM"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Usar la sintaxis Decorator (solo Python)\n",
    "Alternativamente, defina UDF usando la sintaxis de decorador en Python con el tipo de datos que devuelve la función.\n",
    "\n",
    "Ya no podrá llamar a la función Python local (por ejemplo, `decoratorUDF(\"annagray@kaufman.com\")` no funcionará)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11f2c04b-8a11-4cb4-843b-32bc189091db",
     "showTitle": false,
     "title": ""
    },
    "id": "4S6yyJUkXDaN"
   },
   "outputs": [],
   "source": [
    "# Our input/output is a string\n",
    "@udf(\"string\") # spark: indicando el tipo de dato de retorno es string para pyspark\n",
    "def decoratorUDF(email: str) -> str:\n",
    "  return email[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5100aae1-b21c-4b84-be10-3650fded3d1f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHOvZLmuXDaO",
    "outputId": "cb3d970e-1863-45c8-ced9-9b0e69f3f785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|decoratorUDF(email)|\n",
      "+-------------------+\n",
      "|                  k|\n",
      "|                  b|\n",
      "|                  b|\n",
      "|                  j|\n",
      "|                  w|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "salesPath = '../data/sales.parquet'\n",
    "\n",
    "salesDF = spark.read.parquet(salesPath)\n",
    "# Usando la función definida con python\n",
    "salesDF.select(decoratorUDF(col(\"email\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cbfe81be-30fc-450c-945f-4296df10ef6f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZO4ukSuEXDaU",
    "outputId": "14e88763-79cf-44b8-beb8-610bfbf20f13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'decoratorUDF(annagray@kaufman.com)'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La función de python con decorate no funciona para python normalmente\n",
    "decoratorUDF(\"annagray@kaufman.com\") # no funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7a029bc-d8c6-47a3-8f4e-3082872ca516",
     "showTitle": false,
     "title": ""
    },
    "id": "BFz703gnXDaX"
   },
   "source": [
    "### Construcción de funciones para Python y Pyspark\n",
    "Podemos construir una clase de python para que nos permita usar nuestra función tanto para una operación simple como para Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a0aa690-922e-43af-956a-b833a7628890",
     "showTitle": false,
     "title": ""
    },
    "id": "VAiRziv9XDaZ"
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType, DataType\n",
    "class py_or_udf:\n",
    "    def __init__(self, returnType : DataType=StringType()):\n",
    "        self.spark_udf_type = returnType\n",
    "        \n",
    "    def __call__(self, func : Callable):\n",
    "        def wrapped_func(*args, **kwargs):\n",
    "            if any([isinstance(arg, Column) for arg in args]) or \\\n",
    "                any([isinstance(vv, Column) for vv in kwargs.values()]):\n",
    "                return udf(func, self.spark_udf_type)(*args, **kwargs)\n",
    "            else:\n",
    "                return func(*args, **kwargs)\n",
    "            \n",
    "        return wrapped_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3360aec4-0f75-4a08-89fa-e8ceb92a612a",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "YPsmM0SUXDaa",
    "outputId": "2e452a32-b059-444c-f4a7-734790453196"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@py_or_udf(returnType=StringType())\n",
    "def decoratorUDF(email: str) -> str:\n",
    "  return email[0]\n",
    "\n",
    "# This works\n",
    "# assert decoratorUDF(\"annagray@kaufman.com\") == \"a\"\n",
    "\n",
    "decoratorUDF(\"annagray@kaufman.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a70058e-dd5d-4592-b0a8-65b8b45466ca",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phuepD0TXDac",
    "outputId": "280bf127-d464-4b2a-c608-c818e975b06d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|decoratorUDF(email)|\n",
      "+-------------------+\n",
      "|                  k|\n",
      "|                  b|\n",
      "|                  b|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This also works\n",
    "salesDF.select(decoratorUDF(col(\"email\"))).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f5fc41ab-0c7c-4f30-8f7c-b7285e2700c9",
     "showTitle": false,
     "title": ""
    },
    "id": "tsgnUMbnXDae"
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Sort Day Lab\n",
    "1. Define UDF to label day of week\n",
    "1. Apply UDF to label and sort by day of week\n",
    "1. Plot active users by day of week as bar graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d58d9657-9dc1-4c79-af6d-057429232dc7",
     "showTitle": false,
     "title": ""
    },
    "id": "oJ7_GnoFXDag"
   },
   "source": [
    "Start with a DataFrame of the average number of active users by day of week.\n",
    "\n",
    "This was the resulting `df` in a previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1722889a-6435-48f5-97a7-d931ea07c77b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "Jd-7mfSWXDah",
    "outputId": "a7aba3c8-e16b-4f40-f833-97acfb7d5f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|day|avg_users         |\n",
      "+---+------------------+\n",
      "|Sun|281307.5          |\n",
      "|Mon|237582.5          |\n",
      "|Thu|179814.66666666666|\n",
      "|Sat|273175.3333333333 |\n",
      "|Wed|225910.5          |\n",
      "|Fri|251063.66666666666|\n",
      "|Tue|254316.5          |\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct, avg, col, date_format, to_date\n",
    "\n",
    "eventsPath = '../data/events.parquet'\n",
    "\n",
    "df = (spark.read.parquet(eventsPath)\n",
    "  .withColumn(\"ts\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "  .withColumn(\"date\", to_date(\"ts\"))\n",
    "  .groupBy(\"date\").agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n",
    "  .withColumn(\"day\", date_format(col(\"date\"), \"E\"))\n",
    "  .groupBy(\"day\").agg(avg(col(\"active_users\")).alias(\"avg_users\")))\n",
    "\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "981604dc-c627-4ec4-a9e5-4b868d8d896f",
     "showTitle": false,
     "title": ""
    },
    "id": "UX34380FXDai"
   },
   "source": [
    "### 1. Definir UDF para etiquetar el día de la semana\n",
    "Utilice el **`labelDayOfWeek`** proporcionado a continuación para crear el udf **`labelDowUDF`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e720af7e-4c26-4e67-a9ca-04fe3c47543e",
     "showTitle": false,
     "title": ""
    },
    "id": "HeJPgtIUXDak"
   },
   "outputs": [],
   "source": [
    "@py_or_udf(StringType())\n",
    "def labelDayOfWeek(day: str) ->str:\n",
    "  dow = {\"Mon\": \"1\", \"Tue\": \"2\", \"Wed\": \"3\", \"Thu\": \"4\",\n",
    "         \"Fri\": \"5\", \"Sat\": \"6\", \"Sun\": \"7\"}\n",
    "  return dow.get(day) + \"-\" + day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7-Sun'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelDayOfWeek(\"Sun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "65be3486-1c28-4dc8-9e58-d6cddc249e26",
     "showTitle": false,
     "title": ""
    },
    "id": "RGJ6lNWQXDak"
   },
   "source": [
    "### 2. Aplique UDF a la etiqueta y ordene por día de la semana\n",
    "- Actualice la columna del **`day`** aplicando la UDF y reemplazando esta columna\n",
    "- Ordenar por **`day`**\n",
    "- Trazar como gráfico de barras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|  day|         avg_users|\n",
      "+-----+------------------+\n",
      "|7-Sun|          281307.5|\n",
      "|1-Mon|          237582.5|\n",
      "|4-Thu|179814.66666666666|\n",
      "|6-Sat| 273175.3333333333|\n",
      "|3-Wed|          225910.5|\n",
      "+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.withColumn('day',labelDayOfWeek(F.col('day'))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a2e54eab-8966-4cc1-914f-546fb0dde44d",
     "showTitle": false,
     "title": ""
    },
    "id": "dpTzYbrVXDal"
   },
   "source": [
    "### Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7df612a-a492-4612-bcb8-b74d30eb281a",
     "showTitle": false,
     "title": ""
    },
    "id": "k0z7SUhrXDam"
   },
   "source": [
    "- [Udf Pyspark](https://medium.com/@ayplam/developing-pyspark-udfs-d179db0ccc87)\n",
    "- [Pandas UDF](https://medium.com/analytics-vidhya/pyspark-udf-deep-dive-8ae984bfac00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tienes pandas\n",
    "pandasDF = df.toPandas()\n",
    "pandasDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando la función de pyton y pyspark en pandas\n",
    "pandasDF['new']= pandasDF.day.apply(labelDayOfWeek)\n",
    "pandasDF.head()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03 UDFs",
   "notebookOrigID": 4306433353725430,
   "widgets": {}
  },
  "colab": {
   "name": "03 UDFs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
