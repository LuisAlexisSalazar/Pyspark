{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff1a1cfe-155f-4920-bcfa-113274442ad6",
     "showTitle": false,
     "title": ""
    },
    "id": "nB2R34EJXDYu"
   },
   "source": [
    "# Funciones de Usuario\n",
    "1. Definiendo una funcion\n",
    "1. Crear y aplicar UDF\n",
    "1. Registrar UDF para usar en SQL\n",
    "1. Usar la sintaxis Decorator (solo Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\LibreriasPython\\\\spark-3.1.2-bin-hadoop2.7\\\\python\\\\pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DXsAdVd9XGic"
   },
   "outputs": [],
   "source": [
    "#Google Colab\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# inicializamos datos\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-U9Rodi4XUtw",
    "outputId": "8b2d37a3-01b7-44e1-a920-97600dffba05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '01 Archivos Json.ipynb',\n",
       " '02 Conceptos de Spark Streaming.ipynb',\n",
       " '03_UDFs.ipynb',\n",
       " 'Laboratorio Spark Streaming.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "890bd56d-0c55-4d1f-9152-c31263d72820",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxfL5HluXDZa",
    "outputId": "8050bb11-20c8-442e-e84a-de94889b0f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------\n",
      " order_id                | 257437                                                            \n",
      " email                   | kmunoz@powell-duran.com                                           \n",
      " transaction_timestamp   | 1592194221828900                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 1995.0                                                            \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{null, M_PREM_K, Premium King Mattress, 1995.0, 1995.0, 1}]      \n",
      "-RECORD 1------------------------------------------------------------------------------------\n",
      " order_id                | 282611                                                            \n",
      " email                   | bmurillo@hotmail.com                                              \n",
      " transaction_timestamp   | 1592504237604072                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 940.5                                                             \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{NEWBED10, M_STAN_Q, Standard Queen Mattress, 940.5, 1045.0, 1}] \n",
      "-RECORD 2------------------------------------------------------------------------------------\n",
      " order_id                | 257448                                                            \n",
      " email                   | bradley74@gmail.com                                               \n",
      " transaction_timestamp   | 1592200438030141                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 945.0                                                             \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{null, M_STAN_F, Standard Full Mattress, 945.0, 945.0, 1}]       \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesPath = '../data/sales.parquet'\n",
    "\n",
    "salesDF = spark.read.parquet(salesPath)\n",
    "\n",
    "salesDF.show(3, vertical = True,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b88ff625-ffb6-4341-88dd-504db85e561e",
     "showTitle": false,
     "title": ""
    },
    "id": "1e9ZOaLEXDZi"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Con Spark puro\n",
    "\n",
    "\n",
    "Con Spark puro para obtener la primera letra de una cadena del campo `email`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MV9l9LSYYMSy",
    "outputId": "641ab39c-2f2e-43db-c569-94035cd8754a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------\n",
      " order_id                | 257437                                                            \n",
      " email                   | kmunoz@powell-duran.com                                           \n",
      " transaction_timestamp   | 1592194221828900                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 1995.0                                                            \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{null, M_PREM_K, Premium King Mattress, 1995.0, 1995.0, 1}]      \n",
      " FirstLetter             | k                                                                 \n",
      "-RECORD 1------------------------------------------------------------------------------------\n",
      " order_id                | 282611                                                            \n",
      " email                   | bmurillo@hotmail.com                                              \n",
      " transaction_timestamp   | 1592504237604072                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 940.5                                                             \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{NEWBED10, M_STAN_Q, Standard Queen Mattress, 940.5, 1045.0, 1}] \n",
      " FirstLetter             | b                                                                 \n",
      "-RECORD 2------------------------------------------------------------------------------------\n",
      " order_id                | 257448                                                            \n",
      " email                   | bradley74@gmail.com                                               \n",
      " transaction_timestamp   | 1592200438030141                                                  \n",
      " total_item_quantity     | 1                                                                 \n",
      " purchase_revenue_in_usd | 945.0                                                             \n",
      " unique_items            | 1                                                                 \n",
      " items                   | [{null, M_STAN_F, Standard Full Mattress, 945.0, 945.0, 1}]       \n",
      " FirstLetter             | b                                                                 \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# En spark Obteniendo primera letra\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "salesDF.withColumn('FirstLetter', F.substring(F.col('email'),1, 1)).show(3,vertical = True,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b88ff625-ffb6-4341-88dd-504db85e561e",
     "showTitle": false,
     "title": ""
    },
    "id": "1e9ZOaLEXDZi"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Definiendo Funcion\n",
    "\n",
    "\n",
    "Defina una funci√≥n en Python/Scala local para obtener la primera letra de una cadena del campo `email`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9ccc8220-3dd5-4fc0-acc2-07a86d11d93b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ceNnK-WmXDZl",
    "outputId": "bf49a22c-419a-4f1f-dad4-d6d42d398766"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def firstLetterFunction(email):\n",
    "  return email[0]\n",
    "\n",
    "firstLetterFunction(\"annagray@kaufman.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71b74a4c-1a97-4d9f-b555-d9efac2ad829",
     "showTitle": false,
     "title": ""
    },
    "id": "HoUMGSUIXDZ8"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Crear y aplicar UDF\n",
    "Defina una UDF que envuelva la funci√≥n. Esto serializa la funci√≥n y la env√≠a a los ejecutores para poder usarla en nuestro DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "51ef0b40-4802-4ca3-8b38-83e98c4fc6b5",
     "showTitle": false,
     "title": ""
    },
    "id": "7lFTowdVXDZ-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "# Registrar la funci√≥n que hemos creado de python para usarla en pyspark\n",
    "firstLetterUDF = udf(firstLetterFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b3555b1-69e8-40e2-97ea-4f07bded4b36",
     "showTitle": false,
     "title": ""
    },
    "id": "NkpJZ1-EXDZ-"
   },
   "source": [
    "Apply UDF on the `email` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca3468c0-9b5c-4006-8a79-40260a756444",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruFBJvqJXDaA",
    "outputId": "d9b46a49-04a4-4c7e-f24e-5788adcb134a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|firstLetterFunction(email)|\n",
      "+--------------------------+\n",
      "|                         k|\n",
      "|                         b|\n",
      "|                         b|\n",
      "|                         j|\n",
      "|                         w|\n",
      "+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "salesDF.select( firstLetterUDF(col(\"email\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8aca131b-0cfc-48f4-8f54-328bd909298e",
     "showTitle": false,
     "title": ""
    },
    "id": "1DYm0KzdXDaD"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png)Registrar UDF para usar en SQL\n",
    "Registre UDF usando spark.udf.register para crear UDF en el espacio de nombres SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a559e695-0da4-4553-b726-05300edf488c",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRgc24iXXDaJ",
    "outputId": "5fd0f9d5-bd7f-4151-d81f-7ff83c3e741e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.firstLetterFunction(email)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un vista temporal llamada sales\n",
    "salesDF.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# Registrar una funci√≥n en sql para su uso en las queries con el nombre \"sql_udf\"\n",
    "spark.udf.register(\"sql_udf\", firstLetterFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "257c9603-fd46-4205-8356-5bc10ab457bc",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTKbjkOwXDaK",
    "outputId": "b34b3bf6-95c6-469d-8ea1-385574d30629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|firstLetter|\n",
      "+-----------+\n",
      "|          k|\n",
      "|          b|\n",
      "|          b|\n",
      "|          j|\n",
      "|          w|\n",
      "|          e|\n",
      "|          c|\n",
      "+-----------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT sql_udf(email) AS firstLetter FROM sales'\n",
    "\n",
    "\n",
    "df = spark.sql(query)\n",
    "\n",
    "\n",
    "df.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ec90fa3-7799-472e-a458-c5c3733608d3",
     "showTitle": false,
     "title": ""
    },
    "id": "HkJaJFcQXDaM"
   },
   "source": [
    "### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Usar la sintaxis Decorator (solo Python)\n",
    "Alternativamente, defina UDF usando la sintaxis de decorador en Python con el tipo de datos que devuelve la funci√≥n.\n",
    "\n",
    "Ya no podr√° llamar a la funci√≥n Python local (por ejemplo, `decoratorUDF(\"annagray@kaufman.com\")` no funcionar√°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11f2c04b-8a11-4cb4-843b-32bc189091db",
     "showTitle": false,
     "title": ""
    },
    "id": "4S6yyJUkXDaN"
   },
   "outputs": [],
   "source": [
    "# Our input/output is a string\n",
    "@udf(\"string\") # spark: indicando el tipo de dato de retorno es string para pyspark\n",
    "def decoratorUDF(email: str) -> str:\n",
    "  return email[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5100aae1-b21c-4b84-be10-3650fded3d1f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHOvZLmuXDaO",
    "outputId": "cb3d970e-1863-45c8-ced9-9b0e69f3f785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|decoratorUDF(email)|\n",
      "+-------------------+\n",
      "|                  k|\n",
      "|                  b|\n",
      "|                  b|\n",
      "|                  j|\n",
      "|                  w|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "salesPath = '../data/sales.parquet'\n",
    "\n",
    "salesDF = spark.read.parquet(salesPath)\n",
    "# Usando la funci√≥n definida con python\n",
    "salesDF.select(decoratorUDF(col(\"email\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cbfe81be-30fc-450c-945f-4296df10ef6f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZO4ukSuEXDaU",
    "outputId": "14e88763-79cf-44b8-beb8-610bfbf20f13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'decoratorUDF(annagray@kaufman.com)'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La funci√≥n de python con decorate no funciona para python normalmente\n",
    "decoratorUDF(\"annagray@kaufman.com\") # no funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7a029bc-d8c6-47a3-8f4e-3082872ca516",
     "showTitle": false,
     "title": ""
    },
    "id": "BFz703gnXDaX"
   },
   "source": [
    "### Construcci√≥n de funciones para Python y Pyspark\n",
    "Podemos construir una clase de python para que nos permita usar nuestra funci√≥n tanto para una operaci√≥n simple como para Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a0aa690-922e-43af-956a-b833a7628890",
     "showTitle": false,
     "title": ""
    },
    "id": "VAiRziv9XDaZ"
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType, DataType\n",
    "class py_or_udf:\n",
    "    def __init__(self, returnType : DataType=StringType()):\n",
    "        self.spark_udf_type = returnType\n",
    "        \n",
    "    def __call__(self, func : Callable):\n",
    "        def wrapped_func(*args, **kwargs):\n",
    "            if any([isinstance(arg, Column) for arg in args]) or \\\n",
    "                any([isinstance(vv, Column) for vv in kwargs.values()]):\n",
    "                return udf(func, self.spark_udf_type)(*args, **kwargs)\n",
    "            else:\n",
    "                return func(*args, **kwargs)\n",
    "            \n",
    "        return wrapped_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3360aec4-0f75-4a08-89fa-e8ceb92a612a",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "YPsmM0SUXDaa",
    "outputId": "2e452a32-b059-444c-f4a7-734790453196"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@py_or_udf(returnType=StringType())\n",
    "def decoratorUDF(email: str) -> str:\n",
    "  return email[0]\n",
    "\n",
    "# This works\n",
    "# assert decoratorUDF(\"annagray@kaufman.com\") == \"a\"\n",
    "\n",
    "decoratorUDF(\"annagray@kaufman.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a70058e-dd5d-4592-b0a8-65b8b45466ca",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phuepD0TXDac",
    "outputId": "280bf127-d464-4b2a-c608-c818e975b06d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|decoratorUDF(email)|\n",
      "+-------------------+\n",
      "|                  k|\n",
      "|                  b|\n",
      "|                  b|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This also works\n",
    "salesDF.select(decoratorUDF(col(\"email\"))).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f5fc41ab-0c7c-4f30-8f7c-b7285e2700c9",
     "showTitle": false,
     "title": ""
    },
    "id": "tsgnUMbnXDae"
   },
   "source": [
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Sort Day Lab\n",
    "1. Define UDF to label day of week\n",
    "1. Apply UDF to label and sort by day of week\n",
    "1. Plot active users by day of week as bar graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d58d9657-9dc1-4c79-af6d-057429232dc7",
     "showTitle": false,
     "title": ""
    },
    "id": "oJ7_GnoFXDag"
   },
   "source": [
    "Start with a DataFrame of the average number of active users by day of week.\n",
    "\n",
    "This was the resulting `df` in a previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1722889a-6435-48f5-97a7-d931ea07c77b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "Jd-7mfSWXDah",
    "outputId": "a7aba3c8-e16b-4f40-f833-97acfb7d5f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|day|avg_users         |\n",
      "+---+------------------+\n",
      "|Sun|281307.5          |\n",
      "|Mon|237582.5          |\n",
      "|Thu|179814.66666666666|\n",
      "|Sat|273175.3333333333 |\n",
      "|Wed|225910.5          |\n",
      "|Fri|251063.66666666666|\n",
      "|Tue|254316.5          |\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct, avg, col, date_format, to_date\n",
    "\n",
    "eventsPath = '../data/events.parquet'\n",
    "\n",
    "df = (spark.read.parquet(eventsPath)\n",
    "  .withColumn(\"ts\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "  .withColumn(\"date\", to_date(\"ts\"))\n",
    "  .groupBy(\"date\").agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n",
    "  .withColumn(\"day\", date_format(col(\"date\"), \"E\"))\n",
    "  .groupBy(\"day\").agg(avg(col(\"active_users\")).alias(\"avg_users\")))\n",
    "\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "981604dc-c627-4ec4-a9e5-4b868d8d896f",
     "showTitle": false,
     "title": ""
    },
    "id": "UX34380FXDai"
   },
   "source": [
    "### 1. Definir UDF para etiquetar el d√≠a de la semana\n",
    "Utilice el **`labelDayOfWeek`** proporcionado a continuaci√≥n para crear el udf **`labelDowUDF`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e720af7e-4c26-4e67-a9ca-04fe3c47543e",
     "showTitle": false,
     "title": ""
    },
    "id": "HeJPgtIUXDak"
   },
   "outputs": [],
   "source": [
    "@py_or_udf(StringType())\n",
    "def labelDayOfWeek(day: str) ->str:\n",
    "  dow = {\"Mon\": \"1\", \"Tue\": \"2\", \"Wed\": \"3\", \"Thu\": \"4\",\n",
    "         \"Fri\": \"5\", \"Sat\": \"6\", \"Sun\": \"7\"}\n",
    "  return dow.get(day) + \"-\" + day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7-Sun'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelDayOfWeek(\"Sun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "65be3486-1c28-4dc8-9e58-d6cddc249e26",
     "showTitle": false,
     "title": ""
    },
    "id": "RGJ6lNWQXDak"
   },
   "source": [
    "### 2. Aplique UDF a la etiqueta y ordene por d√≠a de la semana\n",
    "- Actualice la columna del **`day`** aplicando la UDF y reemplazando esta columna\n",
    "- Ordenar por **`day`**\n",
    "- Trazar como gr√°fico de barras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|  day|         avg_users|\n",
      "+-----+------------------+\n",
      "|7-Sun|          281307.5|\n",
      "|1-Mon|          237582.5|\n",
      "|4-Thu|179814.66666666666|\n",
      "|6-Sat| 273175.3333333333|\n",
      "|3-Wed|          225910.5|\n",
      "+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.withColumn('day',labelDayOfWeek(F.col('day'))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a2e54eab-8966-4cc1-914f-546fb0dde44d",
     "showTitle": false,
     "title": ""
    },
    "id": "dpTzYbrVXDal"
   },
   "source": [
    "### Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7df612a-a492-4612-bcb8-b74d30eb281a",
     "showTitle": false,
     "title": ""
    },
    "id": "k0z7SUhrXDam"
   },
   "source": [
    "- [Udf Pyspark](https://medium.com/@ayplam/developing-pyspark-udfs-d179db0ccc87)\n",
    "- [Pandas UDF](https://medium.com/analytics-vidhya/pyspark-udf-deep-dive-8ae984bfac00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tienes pandas\n",
    "pandasDF = df.toPandas()\n",
    "pandasDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando la funci√≥n de pyton y pyspark en pandas\n",
    "pandasDF['new']= pandasDF.day.apply(labelDayOfWeek)\n",
    "pandasDF.head()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03 UDFs",
   "notebookOrigID": 4306433353725430,
   "widgets": {}
  },
  "colab": {
   "name": "03 UDFs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
