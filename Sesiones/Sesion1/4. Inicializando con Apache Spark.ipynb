{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c51c7aec-7f7f-4a1a-89c8-cc8bb314e6a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Inicializando con Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7a8b1b4f-bc9e-4d2d-8aac-9d85a4710855",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark Session\n",
    "\n",
    "La sesion de spark es la encargada de ejecutar manipulaciones definidas por el usuario en todo el clúster. Existe una correspondencia uno a uno entre una SparkSession y una aplicación Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84982f76-76b9-4eaf-8ecc-184661cb0a62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creando la sesion de spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('spark_test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "532dbc5a-27b3-4b09-b035-e0db9dcc8ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=8507771068099703#setting/sparkui/0116-000319-jyr4kcnf/driver-5077193363032078589\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8507771068099703#setting/sparkui/0116-000319-jyr4kcnf/driver-5077193363032078589\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# en Databricks, al encender nuestro cluster, este genera una sesion de spark de forma automática\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f97303ca-7d51-4e7b-a867-a687d43361ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "610dde49-be00-4a8a-9937-bfea5b559ba6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+------+\n",
       "|number|\n",
       "+------+\n",
       "|     0|\n",
       "|     1|\n",
       "|     2|\n",
       "|     3|\n",
       "|     4|\n",
       "|     5|\n",
       "|     6|\n",
       "|     7|\n",
       "|     8|\n",
       "|     9|\n",
       "|    10|\n",
       "|    11|\n",
       "|    12|\n",
       "|    13|\n",
       "|    14|\n",
       "|    15|\n",
       "|    16|\n",
       "|    17|\n",
       "|    18|\n",
       "|    19|\n",
       "+------+\n",
       "only showing top 20 rows\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "+------+\n|number|\n+------+\n|     0|\n|     1|\n|     2|\n|     3|\n|     4|\n|     5|\n|     6|\n|     7|\n|     8|\n|     9|\n|    10|\n|    11|\n|    12|\n|    13|\n|    14|\n|    15|\n|    16|\n|    17|\n|    18|\n|    19|\n+------+\nonly showing top 20 rows\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myRange.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15e252bb-9d5e-48fa-8675-83eeaa8efed2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "965e96fd-21c1-49ff-a4f6-88cc901d7307",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "La interface de spark es la que nos permitirá monitorear el progreso de un trabajo. Originalmente, la interfaz de usuario de Spark está disponible en el puerto 4040 del nodo del controlador. Si está ejecutando en modo local, será http://localhost:4040. La interfaz de usuario de Spark muestra información sobre el estado de sus trabajos de Spark, su entorno y el estado del clúster. Es muy útil, especialmente para ajustar y depurar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ea362c0-fdf6-431d-93c6-b0d8491d78db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src='https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781491912201/files/assets/spdg_0206.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "85521a10-b085-4b11-a761-894fc6c8c710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Lectura y Escritura de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1ce7e733-0111-485f-977c-86e670baec71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark permite la **lectura de datos** de archivos de diferentes tipos, en forma general una lectura de datos con spark se realiza de la siguiente manera: \n",
    "\n",
    "\n",
    "\n",
    "<code>DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load()</code>\n",
    "\n",
    "\n",
    "Donde:\n",
    "  - Format: formato a leer\n",
    "  - option: opciones de lectura\n",
    "  - schema: esquema de carga (estable columna y tipo de datos de estos)\n",
    "  - load: ruta a leer\n",
    "\n",
    "\n",
    "Spark tambien establece los siguiente **modos de lectura**:\n",
    "\n",
    "| Read Mode     \t| Description                                                                                                                                                        \t|\n",
    "|---------------\t|--------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| permissive    \t| Establece todos los campos en nulo cuando encuentra un registro corrupto  y coloca todos los registros corruptos en una columna  de cadena llamada _corrupt_record \t|\n",
    "| dropMalformed \t| Elimina registros que contengan data discordante                                                                                                                   \t|\n",
    "| failfast      \t| Falla inmediatamente al encontrar  registros con formato incorrecto                                                                                                \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76d1e82f-9a6d-418d-9b35-7ab1e356a087",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark ofrece la **escritura de datos** de la siguiente manera:\n",
    "\n",
    "<code>DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(\n",
    "  ...).save() </code>\n",
    "  \n",
    "Donde:\n",
    "  - format: formato a escribir el archivo\n",
    "  - option: opciones para la escritura\n",
    "  - partitionBy:  particion del archivo\n",
    "  - save: ruta a guardar el archivo\n",
    "  \n",
    "Tambien se establecen los siguiente modos de escritura:\n",
    "\n",
    "| Write Mode     \t| Description                                                                                       \t|\n",
    "|---------------\t|---------------------------------------------------------------------------------------------------\t|\n",
    "| append        \t| Agrega los archivos de salida a la lista de archivos  que ya existen en esa ubicación             \t|\n",
    "| overwrite     \t| Sobrescribirá por completo cualquier dato que ya  exista en la ruta especificada                  \t|\n",
    "| errorIfExists \t|  Lanza un error y falla la escritura si ya existen datos  o archivos en la ubicación especificada \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "40930020-bd33-44ca-9f98-326eef5ed191",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. CSV\n",
    "-----------------\n",
    "\n",
    "Archivos que contienen información separada comunmente por comas o algún otro separador\n",
    "\n",
    "Estos archivos se leen tradicionalmente de la siguiente manera:\n",
    "\n",
    "<code>spark.read.format(\"csv\")</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9717340a-6548-4075-9dd1-6ed424777825",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# leeremos la data '2010-summary.csv'\n",
    "df_csv = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\") # archivo contiene cabecera?\n",
    "      .option(\"mode\", \"FAILFAST\") # modo lectura\n",
    "      .option(\"inferSchema\", \"true\") # inferir esquema\n",
    "      .load(\"some/path/to/file.csv\")\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo en databricks permite esta función\n",
    "# df_csv.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf3fa9da-99b8-46d7-849e-e51c0d56cad5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# escritura\n",
    "(df_csv.write\n",
    "  .format(\"csv\") # formato escribir\n",
    "  .mode(\"overwrite\") # modo escritura\n",
    "  .option(\"sep\", \"\\t\") # separador del archivo\n",
    "  .save(\"/tmp/my-tsv-file.tsv\") # ruta a escribir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "652fd99c-de2e-45be-a2d1-2175d01a66f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Parquet\n",
    "-----------------\n",
    "\n",
    "Parquet es un almacén de datos orientado a columnas de código abierto que proporciona una variedad de optimizaciones de almacenamiento, especialmente para cargas de trabajo de análisis. \n",
    "\n",
    "Es un formato de archivo que funciona excepcionalmente bien con Apache Spark y, de hecho, es el formato de archivo predeterminado. \n",
    "\n",
    "Se recomienda escribir datos en Parquet para almacenamiento a largo plazo porque la lectura de un archivo de Parquet siempre será más eficiente que JSON o CSV. Otra ventaja de Parquet es que admite tipos complejos. Esto significa que si su columna es una matriz (que fallaría con un archivo CSV, por ejemplo), un mapa o una estructura, aún podrá leer y escribir ese archivo sin problemas. \n",
    "\n",
    "\n",
    "Aquí se explica cómo especificar Parquet como formato de lectura:\n",
    "\n",
    "<code>spark.read.format(\"parquet\")</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aeaa9e08-3f4e-43ee-a6b4-a2e13ed52c93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lectura\n",
    "df_parquet = (spark.read\n",
    "                .format(\"parquet\")\n",
    "                .load(\"/data/flight-data/parquet/2010-summary.parquet\")\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a0658ae-d31b-4bdf-a62e-898f76ee498e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# escritura\n",
    "df_parquet.write.format(\"parquet\").mode(\"overwrite\").save(\"/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a14e1a68-1c1d-4454-ad77-eb9416aa8f0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "4. Inicializando con Apache Spark",
   "notebookOrigID": 2031222202613987,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
